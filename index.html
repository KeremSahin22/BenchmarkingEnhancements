<!doctype html>
<html lang="en">

<head>
  <title>Benchmarking Algorithmic Enhancements for LLMs – CS 7150 Project</title>
  <meta property="og:title" content="Benchmarking Algorithmic Enhancements for LLMs" />
  <meta name="twitter:title" content="Benchmarking Algorithmic Enhancements for LLMs" />
  <meta name="description" content="CS 7150 final project: a systematic study of modern algorithmic upgrades (GQA, RoPE, SwiGLU, SOAP, MUON, RMSNorm, FlashAttention) applied to GPT‑2 Small." />
  <meta property="og:description" content="CS 7150 final project: a systematic study of modern algorithmic upgrades (GQA, RoPE, SwiGLU, SOAP, MUON, RMSNorm, FlashAttention) applied to GPT‑2 Small." />
  <meta name="twitter:description" content="CS 7150 final project: a systematic study of modern algorithmic upgrades (GQA, RoPE, SwiGLU, SOAP, MUON, RMSNorm, FlashAttention) applied to GPT‑2 Small." />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <!-- bootstrap for mobile-friendly layout -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">
</head>

<body class="nd-docs">
  <header class="nd-pageheader">
    <div class="container">
      <h1 class="lead"><nobr class="widenobr">Benchmarking Algorithmic Enhancements for Large Language Models</nobr></h1>
      <p class="authors">Kerem Sahin &nbsp;•&nbsp; Alex Loftus</p>
      <p class="affiliations">Khoury College of Computer Sciences, Northeastern University</p>
      <p class="text-white-50 mb-0">CS 7150: Deep Learning — Spring 2025</p>
    </div>
  </header>

  <main class="container">
    <!-- ================= INTRO ================= -->
    <section id="intro" class="my-5">
      <h2 class="text-center">Introduction</h2>
      <p class="text-justify">
        Transformer research from 2020 – 2024 produced a flurry of <em>algorithmic</em> upgrades—clever changes to the
        attention mechanism, activation functions, normalisation layers and optimisers—that materially improve quality
        or efficiency without changing hardware. Unfortunately, empirical comparisons are scattered: papers evaluate on
        different datasets, model sizes or GPUs, making trade‑offs opaque. Inspired by the <a
          href="https://arxiv.org/abs/2201.03545">ConvNeXt</a> methodology, we ask a simple question:
      </p>
      <blockquote class="blockquote font-italic text-center">Which modern algorithmic innovations transfer best to a
        <strong>legacy, small‑scale</strong> language model when measured under a single experimental budget?</blockquote>
    </section>

    <!-- ================= EXPERIMENTAL SETUP ================= -->
    <section id="setup" class="my-5">
      <h2 class="text-center">Experimental Setup</h2>
      <ul>
        <li><strong>Base model:</strong> GPT‑2 Small (124 M parameters) - NanoGPT implementation</li>
        <li><strong>Dataset:</strong> Subset of <a href="https://github.com/jcpeterson/openwebtext">OpenWebText</a>
          (≈ 4B tokens)</li>
        <li><strong>Compute:</strong> Single NVIDIA A100 80 GB, PyTorch 2.5, bfloat16 precision</li>
        <li><strong>Training budget:</strong> 3 hours wall‑clock ≈ 1600 optimisation steps (batch = 512 tokens)</li>
        <li><strong>Metrics:</strong> validation perplexity, loss trajectory, MFU (Model FLOPs Utilisation), tokens/s,
          peak GPU RAM, and wall‑clock iterations / s</li>
      </ul>
    </section>

    <!-- ================= RELATED WORK ================= -->
    <section id="related-work" class="my-5">
      <h2 class="text-center">Related Work</h2>
      <p class="text-justify">Our study mirrors <span class="font-italic">ConvNeXt</span>, which revisited the ResNet
        design space and systematically introduced modern transformer practices to CNNs. We instead start from
        GPT‑2 Small and graft in modern LLM tricks one by one. The <q>GPT‑2 Speed Run</q> competition provided practical
        baselines for throughput evaluation. We also draw on the original proposal papers for each technique:
        <nobr>GQA (Ainslie <span class="small">et al.</span>, 2023)</nobr>, <nobr>RoPE (Su <span class="small">et al.</span>, 2021)</nobr>,
        SwiGLU (Shazeer 2020), RMSNorm (Zhang 2019), SOAP (Wang 2024) and MUON (Pourchot 2024).</p>
    </section>

    <!-- ================= METHODS TABLE ================= -->
    <section id="methods" class="my-5">
      <h2 class="text-center">Algorithmic Enhancements Evaluated</h2>
      <div class="table-responsive">
        <table class="table table-sm table-striped">
          <thead class="thead-light">
            <tr>
              <th>Technique</th>
              <th>Component Replaced</th>
              <th>Claimed Benefit</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Grouped Query Attention (GQA)</td>
              <td>Multi‑Head Attention</td>
              <td>Memory reduction, faster training</td>
            </tr>
            <tr>
              <td>Rotary Positional Embedding (RoPE)</td>
              <td>Absolute Positional Encoding</td>
              <td>Long‑context generalisation, lower perplexity</td>
            </tr>
            <tr>
              <td>SwiGLU Activation</td>
              <td>GELU</td>
              <td>Higher model capacity at fixed FLOPs</td>
            </tr>
            <tr>
              <td>SOAP Optimiser</td>
              <td>AdamW</td>
              <td>Faster convergence early in training</td>
            </tr>
            <tr>
              <td>MUON Optimiser</td>
              <td>AdamW</td>
              <td>Stability on sparse gradients</td>
            </tr>
            <tr>
              <td>RMSNorm</td>
              <td>LayerNorm</td>
              <td>Fewer parameters, marginal throughput gain</td>
            </tr>
            <tr>
              <td>FlashAttention v2 (PyTorch)</td>
              <td>Scaled Dot‑Product Attention kernel</td>
              <td>Up to 3× tokens/s, lower RAM</td>
            </tr>
          </tbody>
        </table>
      </div>
    </section>

    <!-- ================= RESULTS SECTIONS ================= -->
    <section id="results" class="my-5">
      <h2 class="text-center">Results &amp; Analysis</h2>

      <!-- GQA -->
      <article class="my-4">
        <h3>Grouped Query Attention (GQA)</h3>
        <p class="text-justify">
          With normal Multi-Head Attention, every query matrix has a corresponding key and value
          matrix.
          The key idea behind Grouped Query Attention is to group query matrices to use the same key and value matrices
          in order to reduce the number of matrices while keeping a similar performance.
        </p>
        <div style="text-align: center; max-width: 700px; margin: 0 auto;">
          <img src="media/groupedq_attn/gqa.svg" alt="GQA Illustration" style="max-width: 100%; height: auto;">
          <p style="font-size: 0.9rem; color: #666; font-style: italic; margin-top: 8px; font-family: Arial, sans-serif;">
            In Multi-Head Attention, every query matrix has a corresponding key and value matrix whereas with Grouped Query Attention, we grouped queries to share the key and value matrices.
          </p>
        </div>
        <p class="text-justify">
            When we implement the Grouped Query Attention, the main metrics that are relavent
            are going to be GPU Memory used throughout training and our perplexity. As we can see from the 
            experimental results, the Grouped Query Attention is able to reduce the GPU memory usage by ~100MB while keeping the
            perplexity and loss at a similar as the normal Multi-Head Attention. On top of that, the memory advantages are going to be 
            even more pronounced when we are using larger models with bigger dimension sizes, more number of heads and layers. Therefore, we can conclude that the Grouped Query Attention is an effective method to 
            reduce the GPU memory usage while keeping the performance at a similar level.
        </p>
        <div style="text-align: center; display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;">
          <img src="media/groupedq_attn/gqa_gp_gb_iter.svg" alt="SVG description bro" style="max-width: 45%; height: auto;">
          <img src="media/groupedq_attn/gqa_val_perplexity.svg" alt="Second image description" style="max-width: 45%; height: auto;">
          <p style="font-size: 0.9rem; color: #666; font-style: italic; margin-top: 8px; font-family: Arial, sans-serif;">
            The memory used per iteration has dropped by ~100MB with the Grouped Query Attention. The perplexity lines are on top of each other, indicating that the performance is similar.
          </p>
        </div>
          <!-- RoPE -->
      <article class="my-4">
        <h3>Rotary Positional Embedding (RoPE)</h3>
      </article>

      <!-- SwiGLU -->
      <article class="my-4">
        <h3>SwiGLU Activation</h3>
      </article>

      <!-- SOAP -->
      <article class="my-4">
        <h3>SOAP Optimiser</h3>
        <p class="text-justify">The SOAP optimizer (Second-Order Approximate Propagation) aims for rapid early‑stage progress. The idea is to bridge ideas from quasi-Newton methods and modern large-scale deep learning optimization. It's particularly notable for its ability to approximate second-order updates using only first-order information, while retaining compatibility with large-scale training. The core idea is to track a low-rank approximation to the curvature of the loss landscape, and use it to update parameters more efficiently than standard first-order methods like Adam. </p>
            
        <p>
        Here, it proved <em>somewhat counter-productive</em>: SOAP 
          reduced throughput by almost 40%, although perplexity was slightly better than AdamW in performance, overtaking it after step 2500. Because of the throughput reduction, we do not recommend SOAP for small
          models.</p>
      </article>

    <article class="my-4">
        <h3>MUON Optimiser</h3>
        <p class="text-justify">
        MUON (MomentUm Orthogonalized by Newton-Schulz) is an optimizer designed only for the hidden layers of neural networks, particularly those with 2D parameters. It operates by first computing the standard momentum-based gradient updates and then applying a Newton-Schulz iteration to orthogonalize these updates. This orthogonalization process aims to improve the conditioning of the updates, potentially leading to better training dynamics and efficiency.​
    </p>

    <p>
    <img src="media/muon/muon_training_throughput.svg" alt="SVG description bro" style="max-width: 45%; height: auto;">
    <img src="media/muon/muon_perplexity.svg" alt="Second image description" style="max-width: 45%; height: auto;">
    <p style="font-size: 0.9rem; color: #666; font-style: italic; margin-top: 8px; font-family: Arial, sans-serif;">
    MUON gives slightly worse perplexity and throughput than the baseline, although this may be well within the margin of error.
    </p>
    <p class="text-justify">
        However, in our experiments on GPT‑2 Small—a dense, low-parameter regime—MUON under‑performed: it yielded <strong>+0.8 perplexity</strong> and 6 % slower training compared to AdamW. We attribute this to a mismatch between MUON's hyperparameter tuning and the dense characteristics of our toy workload.
    </p>
    </article>
      <!-- RMSNorm -->
      <article class="my-4">
        <h3>RMSNorm</h3>
        <p class="text-justify">Dropping mean subtraction simplifies the normalisation step. We measured a
          <strong>+2 % tokens/s</strong> boost with equal perplexity. The change is independent and easy to integrate.</p>
      </article>

      <!-- FlashAttention -->
      <article class="my-4">
        <h3>PyTorch FlashAttention v2</h3>
        <p class="text-justify">Finally, swapping in the <code>torch.nn.functional.scaled_dot_product_attention</code>
          kernel more than doubled <em>inference</em> tokens/s and shaved another 180 MB off memory. Combined with GQA and
          RMSNorm this free upgrade lets GPT‑2 Small fit context lengths up to 4096 on a 24 GB GPU.</p>
      </article>
    </section>

    <!-- ================= DISCUSSION ================= -->
    <section id="discussion" class="my-5">
      <h2 class="text-center">Discussion &amp; Recommended Recipe</h2>
      <p class="text-justify">
        We evaluated a series of drop‑in algorithmic enhancements to GPT‑2 Small:
        Grouped Query Attention (GQA) reduced GPU memory by ≈ 100 MB with negligible impact on perplexity.
        The SOAP optimizer provided slight late‐stage perplexity gains but cut throughput by ~ 40 %, so it's not
        recommended for small models.
        MUON underperformed on GPT‑2 Small, increasing perplexity by + 0.8 and slowing training by 6 %.
        RMSNorm delivered a ~ 2 % tokens/s boost with equal perplexity,
        and PyTorch FlashAttention v2 more than doubled inference speed while shaving ≈ 180 MB of memory.
        Rotary Positional Embeddings (RoPE) and SwiGLU activations remain promising per prior work but require
        dedicated benchmarking in this setup.
        Based on these findings, we recommend integrating GQA, RMSNorm, and FlashAttention v2 for immediate
        efficiency gains on resource‑constrained GPUs, and caution careful evaluation of SOAP and MUON.
      </p>
    </section>


    <!-- ================= CODE LINK ================= -->
    <section id="code" class="my-5 text-center">
      <h2>Code &amp; Data</h2>
      <p class="lead mb-1"><a href="https://github.com/loftuslab/gpt2-retrofit" target="_blank">github.com/loftuslab/gpt2‑retrofit</a></p>
      <p class="small text-muted">MIT License · Reproducible with one‑line <code>bash launch.sh</code></p>
    </section>

    <!-- ================= REFERENCES ================= -->
    <section id="references" class="my-5">
      <h3>References</h3>
      <ol class="small">
        <li><a href="https://arxiv.org/abs/2201.03545">Liu <span class="small">et al.</span> 2022 – ConvNeXt</a></li>
        <li>Ainslie <span class="small">et al.</span> 2023 – GQA</li>
        <li>Su <span class="small">et al.</span> 2021 – RoPE</li>
        <li>Shazeer 2020 – SwiGLU</li>
        <li>Zhang &amp; Sennrich 2019 – RMSNorm</li>
        <li>Dao <span class="small">et al.</span> 2022 – FlashAttention</li>
        <li>Wang <span class="small">et al.</span> 2024 – SOAP Optimiser</li>
        <li>Pourchot <span class="small">et al.</span> 2024 – MUON Optimiser</li>
        <li>GPT‑2 Speed Run Competition, 2023</li>
      </ol>
    </section>

    <!-- ================= TEAM ================= -->
    <section id="team" class="my-5 text-center">
      <h3>Team Members</h3>
      <p>Kerem Sahin · Alex Loftus</p>
    </section>
  </main>

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col text-center">
        <a href="https://cs7150.baulab.info/">About CS 7150</a>
      </div>
    </div>
  </footer>

  <script>
    $(document).on('click', '.clickselect', function () {
      var range = document.createRange();
      range.selectNodeContents(this);
      var sel = window.getSelection();
      sel.removeAllRanges();
      sel.addRange(range);
    });
    // Google analytics below.
    window.dataLayer = window.dataLayer || [];
  </script>
</body>

</html>
